<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Billy Cottrell &middot; A Moderately OK Datascience Blog
    
  </title>

  <link rel="stylesheet" href="/styles.css">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="Billy Cottrell" href="/atom.xml">
  <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
</head>

<!--[if lte IE 7]>
<link rel="stylesheet" href="http://latex.codecogs.com/css/ie6.css" type="text/css"/>
<![endif]-->
  <!-- <link rel="stylesheet" type="text/css" href="http://latex.codecogs.com/css/equation-embed.css" />
  <script type="text/javascript"
    src="http://latex.codecogs.com/js/eq_config.js" ></script>
  <script type="text/javascript"
    src="http://latex.codecogs.com/js/eq_editor-lite-19.js" ></script> -->


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">Billy Cottrell</a>
          <small>A Moderately OK Datascience Blog</small></br>
          
          &nbsp;&nbsp;&nbsp;
          <small><a href="/about">About</a></small>
          
          &nbsp;&nbsp;&nbsp;
          <small><a href="/archive">Archive</a></small>
          
          &nbsp;&nbsp;&nbsp;
          <small><a href="/goodreads">Reads</a></small>
          
          &nbsp;&nbsp;&nbsp;
          <small><a href="/index.html">Feed</a></small>
          
        </h3>
      </header>
      
	    

      <main>
        <div class="posts">
  
  <article class="post">
    <h1 class="post-title">
      <a href="/2018/07/23/ScrapingAmazon/">
        Scraping Amazon
      </a>
    </h1>

    <time datetime="2018-07-23T00:00:00-07:00" class="post-date">23 Jul 2018</time>

    <h2 id="scraping-amazon">Scraping Amazon</h2>

<p>For those who want data (who doesn’t?) and who can’t find what they’re looking for on Kaggle, there is often the possibility of scraping the data from the web.  ‘Scraping’ may sound intimidating at first, but it is really just the process of automating the web-search that you might normally do to look something up online.  Fortunately, there are many useful tools in Python to make this easier.  In todays post, I’ll discuss how to scrape Amazon from start to finish.  I’ll also go over an analysis of the data.  This is just going to be a broad overview; more details will be provided in subsequent posts.</p>

<h3 id="motivation">Motivation</h3>

<p>Let’s say I’m selling widgets on Amazon.  Well, actually, I WAS selling widgets on Amazon, so this is not too hard to imagine.  I want to know how to best design my seller’s page.  I.e., how do I optimize the parameters of the page layout in order to maximize profit?</p>

<p>Of course, I could go into one of many Amazon seller chat forums where this precise question is often discussed.  There are of course standard recommendations.  However, I don’t trust humans, I only trust machines.  WWMD? (What would machines do?)</p>

<p>Now, I could consider using the Amazon API to get the data I want.  An API is like filling out a FOIA for a website.  And, like an FOIA, you won’t necessarily be able to get the data you want, when you want it.  You are basically constrained by what they are willing to give you and you have to play by their rules.  In the case of Amazon, one has to sign up as a seller/developer, get a KEY and, most annoyingly, promise to post Amazons junk on your blog.  I would never want to corrupt the purity of this moderately-OK blog, so that was a no-go.</p>

<p>So, we are down to scraping.  The data I want is right on the seller page so I know this should be possible.  In general, if you can see it, you can scrape it.  Of course, the API might actually let you get information that is invisible to the normal user, but we don’t need that anyway.</p>

<p>In order to scrape Amazon I used a combination of Selenium and BeautifulSoup.  These are both Python modules, so, my first two lines of code will look like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">from</span> <span class="nn">selenium</span> <span class="kn">import</span> <span class="n">webdriver</span>
<span class="kn">from</span> <span class="nn">selnium.webdriver.common.keys</span> <span class="kn">import</span> <span class="n">Keys</span>
</code></pre></div></div>

<p>Selenium is a module that let’s python control web-browser just like a human would.  In other words, you are telling the browser step-by-step what buttons to push and what to type in the web-site.</p>

<p>BeautifulSoup, on the other hand, just takes the raw HTML and parses it in order to find useful information.  It also helps format the HTML so that it is human readable.</p>

<p>For websites written in HTML BeautifulSoup alone would have sufficed.  However, many sights are written as JavaScript which is only rendered as HTML when opened.  So, basically, the strategy is to use selenium to open a page and the BeautifulSoup to read it.</p>

<p>Let’s review the basic features of each:</p>

<h3 id="selenium">Selenium</h3>

<p>Again, selenium just allows you to control the web-browser with python.  To use it, you must tell Python exactly where the driver for your browser lives.  First, you need to find where that was installed for your system.  For me it is in ‘Downloads’, for you it might be in Applications.  If you are using Chrome then the file you want will be called ‘chromedriver’.  Once you find the path, you’ll need something like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chromedriver</span> <span class="o">=</span> <span class="s">"path/chromedriver"</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"webdriver.chrome.driver"</span><span class="p">]</span> <span class="o">=</span> <span class="n">chromedriver</span>
</code></pre></div></div>

<p>Now, if you’ve gotten this far and you have a url that you want to read then the rest is easy.  Just fead the url to selenium and convert it to soup!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">driver</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">soup</span><span class="o">=</span><span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">page_source</span><span class="p">,</span><span class="s">'html.parser'</span><span class="p">)</span>
</code></pre></div></div>
<p>Now, ‘soup’ is an HTML like object which can be parsed with bs4.  One useful method that can be called on ‘soup’ is ‘find_all’ which returns all objects with specific properties.  For instance,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s">'span'</span><span class="p">,</span> <span class="n">class</span><span class="o">=</span><span class="s">'...'</span><span class="p">)</span>
</code></pre></div></div>

<p>will find all the objects with header ‘span’ and of class ‘…’.  To figure out what parts of the page you’re actually looking for it is convenient to go and open the browser and use the ‘DevelopmentTools’ feature.  This will allow you to display the HTML of whatever you click or hover over.  You then need to put those specs into the ‘find_all’ method.  There are more tools for parsing, but that is all that we will need for the moment.</p>

<h3 id="beautiful-soup">Beautiful Soup</h3>

<p>(brief description of bs)</p>

<h2 id="problems">Problems</h2>

<p>Now we are ready to sccrape!</p>

<p>Of course, in datascience things are never that easy.  Amazon doesn’t want us scraping them.  To see what, precisely, they do not like one can consult the .robots file.  In any case, we need to adopt some defensive measures.</p>

<p>1) Rotate IPs with <em>expressvpn</em>
2) Rotate user agent.
3) Insert randomized sleeps.
4) Randomize order of search, etc.</p>

<p>…need more time to finish…</p>

<p>…next blog will be about the data.</p>

  </article>
  
  <article class="post">
    <h1 class="post-title">
      <a href="/2018/07/11/Project-Turnstile/">
        Project Turnstile
      </a>
    </h1>

    <time datetime="2018-07-11T00:00:00-07:00" class="post-date">11 Jul 2018</time>

    <p>Todays blog is about my first project for the Metis datascience bootcamp (SF 2018).
The premise of this project is that a group ‘WomenTechWomenYes’ (WTWY) is trying
to distribute flyers in support of their upcoming summer gala.  The group would like to find a good plan for when and where to distribute.  ‘We’ are being hired to help them develop this strategy.</p>

<p>The group is based in New York and so we have lots of subway turnstile data to work with.  This will form the basis for our analysis.  The basic idea is that the more people leaving the subway at a given time the more people will accept fliers.  We are particularly interested in exits.  No-one can stop a New Yorker trying to catch a subway.</p>

<p>We are also interested in other factors which might affect when someone will accept a flier.  Since the target group is women in tech, we will want to focus on regions with more women.  Tech workers like coffee, so it might also help to look at stations near many coffee shops.  We could also look at tech companies, or any number of other factors.</p>

<p>What we really want is an easily replicable pipeline that will allow us to look for the locations of <em>any</em> type of establishment and then compute the distances to nearby subway stations.  Fortunately, the google places api will help us do this.  Basically, we will be able to enter in a plane English keyword, (e.g., ‘Coffee Shops’ or ‘Apple Stores’) and the goolgeplaces api will just spit out a list of addresses.  We can then convert these into a form that geopandas can recognize and make some nice plots.
Our goal is to do this for a few different keywords and then form an aggregate score based on the distances between subway stations and the output of our keyword searches.</p>

<p>The code for this project may be found here: <a href="https://github.com/williamcottrell72/project_benson">repo</a>.  My team consisted of myself, Xu, Alan, Auste and Chelan.  Much of the code in the repo is theirs, though I plan to clean it up and synthesize it better when I have more time.</p>

<p>#Implementation</p>

<p>##MTA Data</p>

<p>The MTA data is freely available from the MTA website.  The data is organized by week, with the startdate of the week being used in the page url.  This structure allows one to scrape multiple weeks using code of the form:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scrape</span><span class="p">(</span><span class="n">week_nums</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s">"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt"</span>
    <span class="n">dfs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">week_num</span> <span class="ow">in</span> <span class="n">week_nums</span><span class="p">:</span>
        <span class="n">file_url</span> <span class="o">=</span> <span class="n">url</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">week_num</span><span class="p">)</span>
        <span class="n">dfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_url</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dfs</span><span class="p">)</span>
</code></pre></div></div>

<p>Here, ‘week_nums’ is a list of properly formatted week numbers we’ve generated using the python datetime module.</p>

<p>Since there is quiet a bit of data on this site we should be a bit selective about how we scrape.  We are only interested in one particular month, the month before the gala, and so we could only scrape that month in some number of years.  In fact, this is what I tried at first.  Unfortunately, that led to an annoying problem. It turns out that the data only contains information about the cumulative number of exits up to a given time.  The easiest way to get the flux is to apply a .diff operatoin on a pandas dataframe.  However, if we have rows separated by a whole year the .diff will give an anomolously large value, which we will then have to figure out what to do with.</p>

<p>An easier way to deal with this is to scrape 3 months for each year, the month you care about and the months preceding and following.  Then, apply diff and drop the two extra months.  This will leave us with better data and we won’t have to scrape everything.  (Of course, we really just need two extra weeks, not two extra months.)</p>

<p>After scraping the data we want to process it into a useful form.  We want to optimate ‘flier acceptance likelihood’ and there are two independent variables ‘place’ (MTA station) and ‘time’.  Obviously, we don’t want to just optimize both ‘place’ and ‘time’ simultaneously since then we will only have one place to go at one time per week.  So, what do we want to do? Maybe optimize  ‘time’ when averaging over ‘place’ or ‘place’ averaging over ‘time’?  Or, should we optimize one and then the other, or visa versa???</p>

<p>Of course, it is good to do all these things, but, we should keep in mind that time is really the most valuable commodity here.  The ‘time’ is really fixed by the volunteers schedules.  There are perhaps many potential volunteers who can only help at particular times.  Perhaps there are others that are free.  However, we get a huge benefit from bringing in volunteers whom otherwise would not be available.  And, when they are avaiable, we should be able to determine the best station.  Thus, what we really want is a function that reports back the best subway station as a function of time.</p>

<p>This is implemented in the ‘Clean_and_Process.ipynb’ file in the function ‘activity_by_time’.  For the purpose of making maps, however, we just want to take some stations that ‘on average’ are the best.  This list is constructed in ‘clean_df_cum’.  Specifically, we focus on the afternoon to avoid trapping too many people on their way to work.</p>

<p>The final results are heavily centered around Broadway, Times Square, etc.  A natural worry is that these are tourist and not likely to join the gala.  If there were more time this issue could be investigated further.</p>

<p>##Maps</p>

<p>As mentioned, the googleplaces api allows us to easily search for places based on a keyword.  To use this, we need to register and get a key:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">API_KEYAPI_KEY</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>
<span class="n">google_places</span> <span class="o">=</span> <span class="n">GooglePlaces</span><span class="p">(</span><span class="n">API_KEY</span><span class="p">)</span>
<span class="n">gmaps</span> <span class="o">=</span> <span class="n">googlemaps</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="n">API_KEY</span><span class="p">)</span>
</code></pre></div></div>

<p>These functions are very user friendly since they allows us to type in normal English.  If you want the list of American restaurants with 3200m of Shenzhen, China for instance</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query_result</span><span class="o">=</span><span class="n">google_places</span><span class="o">.</span><span class="n">nearby_search</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="s">'Shenzhen, China'</span><span class="p">,</span><span class="n">keyword</span> <span class="o">=</span> <span class="s">'American Restaurant'</span><span class="p">,</span> <span class="n">radius</span> <span class="o">=</span> <span class="mi">3200</span><span class="p">)</span>
</code></pre></div></div>

<p>We want a list of Starbucks instead.</p>

<p>We can also use this method on the most popular MTA stations determined above.  Googleplaces allows us to get geojson codes for the stations, which can be fed into geopandas.  Overlaying this on a map showing gender percentages we get:</p>

<p><img src="https://drive.google.com/uc?id=14TJGSCWEjuv5xRmt7oAkgR_IrdiWZY3i" /></p>

<p>##Final Scores</p>

<p>Combining the various datasets we get a final score.  The choice of function used to combine these datasets if fairly arbitrary.  Ideally, more time would have been spent in deciding how to combine the various factors.  Oh well.  I have no more time to continue.  I hope you liked reading this.  Signing off.</p>

  </article>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page4">Older</a>
  
  
    <a class="pagination-item newer" href="/page2">Newer</a>
  
</div>

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2018-11-24T19:18:43-08:00">2018</time>. All rights reserved.
        </small>
      </footer>
    </div>








    
  </body>
</html>
